{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn\n",
    "import autosklearn.classification as classifier\n",
    "import sklearn.datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AutoSklearnClassifier in module autosklearn.estimators:\n",
      "\n",
      "class AutoSklearnClassifier(AutoSklearnEstimator, sklearn.base.ClassifierMixin)\n",
      " |  AutoSklearnClassifier(time_left_for_this_task=3600, per_run_time_limit=None, initial_configurations_via_metalearning=25, ensemble_size: 'int | None' = None, ensemble_class: \"Type[AbstractEnsemble] | Literal['default'] | None\" = 'default', ensemble_kwargs: 'Dict[str, Any] | None' = None, ensemble_nbest=50, max_models_on_disc=50, seed=1, memory_limit=3072, include: 'Optional[Dict[str, List[str]]]' = None, exclude: 'Optional[Dict[str, List[str]]]' = None, resampling_strategy='holdout', resampling_strategy_arguments=None, tmp_folder=None, delete_tmp_folder_after_terminate=True, n_jobs: 'Optional[int]' = None, dask_client: 'Optional[dask.distributed.Client]' = None, disable_evaluator_output=False, get_smac_object_callback=None, smac_scenario_args=None, logging_config=None, metadata_directory=None, metric: 'Scorer | Sequence[Scorer] | None' = None, scoring_functions: 'Optional[List[Scorer]]' = None, load_models: 'bool' = True, get_trials_callback: 'SMACCallback | None' = None, dataset_compression: 'Union[bool, Mapping[str, Any]]' = True, allow_string_features: 'bool' = True)\n",
      " |  \n",
      " |  This class implements the classification task.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AutoSklearnClassifier\n",
      " |      AutoSklearnEstimator\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X, y, X_test=None, y_test=None, feat_type=None, dataset_name=None)\n",
      " |      Fit *auto-sklearn* to given training set (X, y).\n",
      " |      \n",
      " |      Fit both optimizes the machine learning models and builds an ensemble\n",
      " |      out of them.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target classes.\n",
      " |      \n",
      " |      X_test : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Test data input samples. Will be used to save test predictions for\n",
      " |          all models. This allows to evaluate the performance of Auto-sklearn\n",
      " |          over time.\n",
      " |      \n",
      " |      y_test : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          Test data target classes. Will be used to calculate the test error\n",
      " |          of all models. This allows to evaluate the performance of\n",
      " |          Auto-sklearn over time.\n",
      " |      \n",
      " |      feat_type : list, optional (default=None)\n",
      " |          List of str of `len(X.shape[1])` describing the attribute type.\n",
      " |          Possible types are `Categorical` and `Numerical`. `Categorical`\n",
      " |          attributes will be automatically One-Hot encoded. The values\n",
      " |          used for a categorical attribute must be integers, obtained for\n",
      " |          example by `sklearn.preprocessing.LabelEncoder\n",
      " |          <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>`_.\n",
      " |      \n",
      " |      dataset_name : str, optional (default=None)\n",
      " |          Create nicer output. If None, a string will be determined by the\n",
      " |          md5 hash of the dataset.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  predict(self, X, batch_size=None, n_jobs=1)\n",
      " |      Predict classes for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_labels]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_proba(self, X, batch_size=None, n_jobs=1)\n",
      " |      Predict probabilities of classes for all samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |      \n",
      " |      batch_size : int (optional)\n",
      " |          Number of data points to predict for (predicts all points at once\n",
      " |          if ``None``.\n",
      " |      n_jobs : int\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples, n_classes] or [n_samples, n_labels]\n",
      " |          The predicted class probabilities.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from AutoSklearnEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, time_left_for_this_task=3600, per_run_time_limit=None, initial_configurations_via_metalearning=25, ensemble_size: 'int | None' = None, ensemble_class: \"Type[AbstractEnsemble] | Literal['default'] | None\" = 'default', ensemble_kwargs: 'Dict[str, Any] | None' = None, ensemble_nbest=50, max_models_on_disc=50, seed=1, memory_limit=3072, include: 'Optional[Dict[str, List[str]]]' = None, exclude: 'Optional[Dict[str, List[str]]]' = None, resampling_strategy='holdout', resampling_strategy_arguments=None, tmp_folder=None, delete_tmp_folder_after_terminate=True, n_jobs: 'Optional[int]' = None, dask_client: 'Optional[dask.distributed.Client]' = None, disable_evaluator_output=False, get_smac_object_callback=None, smac_scenario_args=None, logging_config=None, metadata_directory=None, metric: 'Scorer | Sequence[Scorer] | None' = None, scoring_functions: 'Optional[List[Scorer]]' = None, load_models: 'bool' = True, get_trials_callback: 'SMACCallback | None' = None, dataset_compression: 'Union[bool, Mapping[str, Any]]' = True, allow_string_features: 'bool' = True)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      time_left_for_this_task : int, optional (default=3600)\n",
      " |          Time limit in seconds for the search of appropriate\n",
      " |          models. By increasing this value, *auto-sklearn* has a higher\n",
      " |          chance of finding better models.\n",
      " |      \n",
      " |      per_run_time_limit : int, optional (default=1/10 of time_left_for_this_task)\n",
      " |          Time limit for a single call to the machine learning model.\n",
      " |          Model fitting will be terminated if the machine learning\n",
      " |          algorithm runs over the time limit. Set this value high enough so\n",
      " |          that typical machine learning algorithms can be fit on the\n",
      " |          training data.\n",
      " |      \n",
      " |      initial_configurations_via_metalearning : int, optional (default=25)\n",
      " |          Initialize the hyperparameter optimization algorithm with this\n",
      " |          many configurations which worked well on previously seen\n",
      " |          datasets. Disable if the hyperparameter optimization algorithm\n",
      " |          should start from scratch.\n",
      " |      \n",
      " |      ensemble_size : int, optional\n",
      " |          Number of models added to the ensemble built by *Ensemble\n",
      " |          selection from libraries of models*. Models are drawn with\n",
      " |          replacement. If set to ``0`` no ensemble is fit.\n",
      " |      \n",
      " |          Deprecated - will be removed in Auto-sklearn 0.16. Please pass\n",
      " |          this argument via ``ensemble_kwargs={\"ensemble_size\": int}``\n",
      " |          if you want to change the ensemble size for ensemble selection.\n",
      " |      \n",
      " |      ensemble_class : Type[AbstractEnsemble] | \"default\", optional (default=\"default\")\n",
      " |          Class implementing the post-hoc ensemble algorithm. Set to\n",
      " |          ``None`` to disable ensemble building or use :class:`SingleBest`\n",
      " |          to obtain only use the single best model instead of an\n",
      " |          ensemble.\n",
      " |      \n",
      " |          If set to \"default\" it will use :class:`EnsembleSelection` for\n",
      " |          single-objective problems and :class:`MultiObjectiveDummyEnsemble`\n",
      " |          for multi-objective problems.\n",
      " |      \n",
      " |      ensemble_kwargs : Dict, optional\n",
      " |          Keyword arguments that are passed to the ensemble class upon\n",
      " |          initialization.\n",
      " |      \n",
      " |      ensemble_nbest : int, optional (default=50)\n",
      " |          Only consider the ``ensemble_nbest`` models when building an\n",
      " |          ensemble. This is inspired by a concept called library pruning\n",
      " |          introduced in `Getting Most out of Ensemble Selection`. This\n",
      " |          is independent of the ``ensemble_class`` argument and this\n",
      " |          pruning step is done prior to constructing an ensemble.\n",
      " |      \n",
      " |      max_models_on_disc: int, optional (default=50),\n",
      " |          Defines the maximum number of models that are kept in the disc.\n",
      " |          The additional number of models are permanently deleted. Due to the\n",
      " |          nature of this variable, it sets the upper limit on how many models\n",
      " |          can be used for an ensemble.\n",
      " |          It must be an integer greater or equal than 1.\n",
      " |          If set to None, all models are kept on the disc.\n",
      " |      \n",
      " |      seed : int, optional (default=1)\n",
      " |          Used to seed SMAC. Will determine the output file names.\n",
      " |      \n",
      " |      memory_limit : int, optional (3072)\n",
      " |          Memory limit in MB for the machine learning algorithm.\n",
      " |          `auto-sklearn` will stop fitting the machine learning algorithm if\n",
      " |          it tries to allocate more than ``memory_limit`` MB.\n",
      " |      \n",
      " |          **Important notes:**\n",
      " |      \n",
      " |          * If ``None`` is provided, no memory limit is set.\n",
      " |          * In case of multi-processing, ``memory_limit`` will be *per job*, so the total usage is\n",
      " |            ``n_jobs x memory_limit``.\n",
      " |          * The memory limit also applies to the ensemble creation process.\n",
      " |      \n",
      " |      include : Optional[Dict[str, List[str]]] = None\n",
      " |          If None, all possible algorithms are used.\n",
      " |      \n",
      " |          Otherwise, specifies a step and the components that are included in search.\n",
      " |          See ``/pipeline/components/<step>/*`` for available components.\n",
      " |      \n",
      " |          Incompatible with parameter ``exclude``.\n",
      " |      \n",
      " |          **Possible Steps**:\n",
      " |      \n",
      " |          * ``\"data_preprocessor\"``\n",
      " |          * ``\"balancing\"``\n",
      " |          * ``\"feature_preprocessor\"``\n",
      " |          * ``\"classifier\"`` - Only for when when using ``AutoSklearnClasssifier``\n",
      " |          * ``\"regressor\"`` - Only for when when using ``AutoSklearnRegressor``\n",
      " |      \n",
      " |          **Example**:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              include = {\n",
      " |                  'classifier': [\"random_forest\"],\n",
      " |                  'feature_preprocessor': [\"no_preprocessing\"]\n",
      " |              }\n",
      " |      \n",
      " |      exclude : Optional[Dict[str, List[str]]] = None\n",
      " |          If None, all possible algorithms are used.\n",
      " |      \n",
      " |          Otherwise, specifies a step and the components that are excluded from search.\n",
      " |          See ``/pipeline/components/<step>/*`` for available components.\n",
      " |      \n",
      " |          Incompatible with parameter ``include``.\n",
      " |      \n",
      " |          **Possible Steps**:\n",
      " |      \n",
      " |          * ``\"data_preprocessor\"``\n",
      " |          * ``\"balancing\"``\n",
      " |          * ``\"feature_preprocessor\"``\n",
      " |          * ``\"classifier\"`` - Only for when when using ``AutoSklearnClasssifier``\n",
      " |          * ``\"regressor\"`` - Only for when when using ``AutoSklearnRegressor``\n",
      " |      \n",
      " |          **Example**:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              exclude = {\n",
      " |                  'classifier': [\"random_forest\"],\n",
      " |                  'feature_preprocessor': [\"no_preprocessing\"]\n",
      " |              }\n",
      " |      \n",
      " |      resampling_strategy : str | BaseCrossValidator | _RepeatedSplits | BaseShuffleSplit = \"holdout\"\n",
      " |          How to to handle overfitting, might need to use ``resampling_strategy_arguments``\n",
      " |          if using ``\"cv\"`` based method or a Splitter object.\n",
      " |      \n",
      " |          * **Options**\n",
      " |              *   ``\"holdout\"`` - Use a 67:33 (train:test) split\n",
      " |              *   ``\"cv\"``: perform cross validation, requires \"folds\" in ``resampling_strategy_arguments``\n",
      " |              *   ``\"holdout-iterative-fit\"`` - Same as \"holdout\" but iterative fit where possible\n",
      " |              *   ``\"cv-iterative-fit\"``: Same as \"cv\" but iterative fit where possible\n",
      " |              *   ``\"partial-cv\"``: Same as \"cv\" but uses intensification.\n",
      " |              *   ``BaseCrossValidator`` - any BaseCrossValidator subclass (found in scikit-learn model_selection module)\n",
      " |              *   ``_RepeatedSplits`` - any _RepeatedSplits subclass (found in scikit-learn model_selection module)\n",
      " |              *   ``BaseShuffleSplit`` - any BaseShuffleSplit subclass (found in scikit-learn model_selection module)\n",
      " |      \n",
      " |          If using a Splitter object that relies on the dataset retaining it's current\n",
      " |          size and order, you will need to look at the ``dataset_compression`` argument\n",
      " |          and ensure that ``\"subsample\"`` is not included in the applied compression\n",
      " |          ``\"methods\"`` or disable it entirely with ``False``.\n",
      " |      \n",
      " |      resampling_strategy_arguments : Optional[Dict] = None\n",
      " |          Additional arguments for ``resampling_strategy``, this is required if\n",
      " |          using a ``cv`` based strategy. The default arguments if left as ``None``\n",
      " |          are:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              {\n",
      " |                  \"train_size\": 0.67,     # The size of the training set\n",
      " |                  \"shuffle\": True,        # Whether to shuffle before splitting data\n",
      " |                  \"folds\": 5              # Used in 'cv' based resampling strategies\n",
      " |              }\n",
      " |      \n",
      " |          If using a custom splitter class, which takes ``n_splits`` such as\n",
      " |          `PredefinedSplit <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn-model-selection-kfold>`_,\n",
      " |          the value of ``\"folds\"`` will be used.\n",
      " |      \n",
      " |      tmp_folder : string, optional (None)\n",
      " |          folder to store configuration output and log files, if ``None``\n",
      " |          automatically use ``/tmp/autosklearn_tmp_$pid_$random_number``\n",
      " |      \n",
      " |      delete_tmp_folder_after_terminate: bool, optional (True)\n",
      " |          remove tmp_folder, when finished. If tmp_folder is None\n",
      " |          tmp_dir will always be deleted\n",
      " |      \n",
      " |      n_jobs : int, optional, experimental\n",
      " |          The number of jobs to run in parallel for ``fit()``. ``-1`` means\n",
      " |          using all processors.\n",
      " |      \n",
      " |          **Important notes**:\n",
      " |      \n",
      " |          * By default, Auto-sklearn uses one core.\n",
      " |          * Ensemble building is not affected by ``n_jobs`` but can be controlled by the number\n",
      " |            of models in the ensemble.\n",
      " |          * ``predict()`` is not affected by ``n_jobs`` (in contrast to most scikit-learn models)\n",
      " |          * If ``dask_client`` is ``None``, a new dask client is created.\n",
      " |      \n",
      " |      dask_client : dask.distributed.Client, optional\n",
      " |          User-created dask client, can be used to start a dask cluster and then\n",
      " |          attach auto-sklearn to it.\n",
      " |      \n",
      " |      disable_evaluator_output: bool or list, optional (False)\n",
      " |          If True, disable model and prediction output. Cannot be used\n",
      " |          together with ensemble building. ``predict()`` cannot be used when\n",
      " |          setting this True. Can also be used as a list to pass more\n",
      " |          fine-grained information on what to save. Allowed elements in the\n",
      " |          list are:\n",
      " |      \n",
      " |          * ``'y_optimization'`` : do not save the predictions for the\n",
      " |            optimization set, which would later on be used to build an ensemble.\n",
      " |      \n",
      " |          * ``model`` : do not save any model files\n",
      " |      \n",
      " |      smac_scenario_args : dict, optional (None)\n",
      " |          Additional arguments inserted into the scenario of SMAC. See the\n",
      " |          `SMAC documentation <https://automl.github.io/SMAC3/main/api/smac.scenario.scenario.html#module-smac.scenario.scenario>`_\n",
      " |          for a list of available arguments.\n",
      " |      \n",
      " |      get_smac_object_callback : callable\n",
      " |          Callback function to create an object of class\n",
      " |          `smac.optimizer.smbo.SMBO <https://automl.github.io/SMAC3/main/api/smac.optimizer.smbo.html>`_.\n",
      " |          The function must accept the arguments ``scenario_dict``,\n",
      " |          ``instances``, ``num_params``, ``runhistory``, ``seed`` and ``ta``.\n",
      " |          This is an advanced feature. Use only if you are familiar with\n",
      " |          `SMAC <https://automl.github.io/SMAC3/main/index.html>`_.\n",
      " |      \n",
      " |      logging_config : dict, optional (None)\n",
      " |          dictionary object specifying the logger configuration. If None,\n",
      " |          the default logging.yaml file is used, which can be found in\n",
      " |          the directory ``util/logging.yaml`` relative to the installation.\n",
      " |      \n",
      " |      metadata_directory : str, optional (None)\n",
      " |          path to the metadata directory. If None, the default directory\n",
      " |          (autosklearn.metalearning.files) is used.\n",
      " |      \n",
      " |      metric : Scorer, optional (None)\n",
      " |          An instance of :class:`autosklearn.metrics.Scorer` as created by\n",
      " |          :meth:`autosklearn.metrics.make_scorer`. These are the `Built-in\n",
      " |          Metrics`_.\n",
      " |          If None is provided, a default metric is selected depending on the task.\n",
      " |      \n",
      " |      scoring_functions : List[Scorer], optional (None)\n",
      " |          List of scorers which will be calculated for each pipeline and results will be\n",
      " |          available via ``cv_results``\n",
      " |      \n",
      " |      load_models : bool, optional (True)\n",
      " |          Whether to load the models after fitting Auto-sklearn.\n",
      " |      \n",
      " |      get_trials_callback: callable\n",
      " |          A callable with the following definition.\n",
      " |      \n",
      " |          * (smac.SMBO, smac.RunInfo, smac.RunValue, time_left: float) -> bool | None\n",
      " |      \n",
      " |          This will be called after SMAC, the underlying optimizer for autosklearn,\n",
      " |          finishes training each run.\n",
      " |      \n",
      " |          You can use this to record your own information about the optimization\n",
      " |          process. You can also use this to enable a early stopping based on some\n",
      " |          critera.\n",
      " |      \n",
      " |          See the example:\n",
      " |          :ref:`Early Stopping And Callbacks <sphx_glr_examples_40_advanced_example_early_stopping_and_callbacks.py>`.\n",
      " |      \n",
      " |      dataset_compression: Union[bool, Mapping[str, Any]] = True\n",
      " |          We compress datasets so that they fit into some predefined amount of memory.\n",
      " |          Currently this does not apply to dataframes or sparse arrays, only to raw\n",
      " |          numpy arrays.\n",
      " |      \n",
      " |          **NOTE** - If using a custom ``resampling_strategy`` that relies on specific\n",
      " |          size or ordering of data, this must be disabled to preserve these properties.\n",
      " |      \n",
      " |          You can disable this entirely by passing ``False`` or leave as the default\n",
      " |          ``True`` for configuration below.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              {\n",
      " |                  \"memory_allocation\": 0.1,\n",
      " |                  \"methods\": [\"precision\", \"subsample\"]\n",
      " |              }\n",
      " |      \n",
      " |          You can also pass your own configuration with the same keys and choosing\n",
      " |          from the available ``\"methods\"``.\n",
      " |      \n",
      " |          The available options are described here:\n",
      " |      \n",
      " |          * **memory_allocation**\n",
      " |              By default, we attempt to fit the dataset into ``0.1 * memory_limit``.\n",
      " |              This float value can be set with ``\"memory_allocation\": 0.1``.\n",
      " |              We also allow for specifying absolute memory in MB, e.g. 10MB is\n",
      " |              ``\"memory_allocation\": 10``.\n",
      " |      \n",
      " |              The memory used by the dataset is checked after each reduction method is\n",
      " |              performed. If the dataset fits into the allocated memory, any further\n",
      " |              methods listed in ``\"methods\"`` will not be performed.\n",
      " |      \n",
      " |              For example, if ``methods: [\"precision\", \"subsample\"]`` and the\n",
      " |              ``\"precision\"`` reduction step was enough to make the dataset fit into\n",
      " |              memory, then the ``\"subsample\"`` reduction step will not be performed.\n",
      " |      \n",
      " |          * **methods**\n",
      " |              We provide the following methods for reducing the dataset size.\n",
      " |              These can be provided in a list and are performed in the order as given.\n",
      " |      \n",
      " |              *   ``\"precision\"`` - We reduce floating point precision as follows:\n",
      " |                  *   ``np.float128 -> np.float64``\n",
      " |                  *   ``np.float96 -> np.float64``\n",
      " |                  *   ``np.float64 -> np.float32``\n",
      " |      \n",
      " |              *   ``subsample`` - We subsample data such that it **fits directly into\n",
      " |                  the memory allocation** ``memory_allocation * memory_limit``.\n",
      " |                  Therefore, this should likely be the last method listed in\n",
      " |                  ``\"methods\"``.\n",
      " |                  Subsampling takes into account classification labels and stratifies\n",
      " |                  accordingly. We guarantee that at least one occurrence of each\n",
      " |                  label is included in the sampled set.\n",
      " |      \n",
      " |      allow_string_features: bool = True\n",
      " |          Whether autosklearn should process string features. By default the\n",
      " |          textpreprocessing is enabled.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      cv_results_ : dict of numpy (masked) ndarrays\n",
      " |          A dict with keys as column headers and values as columns, that can be\n",
      " |          imported into a pandas ``DataFrame``.\n",
      " |      \n",
      " |          Not all keys returned by scikit-learn are supported yet.\n",
      " |      \n",
      " |      performance_over_time_ : pandas.core.frame.DataFrame\n",
      " |          A ``DataFrame`` containing the models performance over time data. Can be\n",
      " |          used for plotting directly. Please refer to the example\n",
      " |          :ref:`Train and Test Inputs <sphx_glr_examples_40_advanced_example_pandas_train_test.py>`.\n",
      " |  \n",
      " |  build_automl(self)\n",
      " |  \n",
      " |  fit_ensemble(self, y, task: 'int' = None, precision: 'Literal[16, 21, 64]' = 32, dataset_name: 'Optional[str]' = None, ensemble_size: 'int | None' = None, ensemble_kwargs: 'Optional[Dict[str, Any]]' = None, ensemble_nbest: 'Optional[int]' = None, ensemble_class: \"Type[AbstractEnsemble] | Literal['default'] | None\" = 'default', metric: 'Scorer | Sequence[Scorer] | None' = None)\n",
      " |      Fit an ensemble to models trained during an optimization process.\n",
      " |      \n",
      " |      All parameters are ``None`` by default. If no other value is given,\n",
      " |      the default values which were set in a call to ``fit()`` are used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like\n",
      " |          Target values.\n",
      " |      \n",
      " |      task : int\n",
      " |          A constant from the module ``autosklearn.constants``. Determines\n",
      " |          the task type (binary classification, multiclass classification,\n",
      " |          multilabel classification or regression).\n",
      " |      \n",
      " |      precision : int\n",
      " |          Numeric precision used when loading ensemble data. Can be either\n",
      " |          ``16``, ``32`` or ``64``.\n",
      " |      \n",
      " |      dataset_name : str\n",
      " |          Name of the current data set.\n",
      " |      \n",
      " |      ensemble_size : int, optional\n",
      " |          Number of models added to the ensemble built by *Ensemble\n",
      " |          selection from libraries of models*. Models are drawn with\n",
      " |          replacement. If set to ``0`` no ensemble is fit.\n",
      " |      \n",
      " |          Deprecated - will be removed in Auto-sklearn 0.16. Please pass\n",
      " |          this argument via ``ensemble_kwargs={\"ensemble_size\": int}``\n",
      " |          if you want to change the ensemble size for ensemble selection.\n",
      " |      \n",
      " |      ensemble_kwargs : Dict, optional\n",
      " |          Keyword arguments that are passed to the ensemble class upon\n",
      " |          initialization.\n",
      " |      \n",
      " |      ensemble_nbest : int\n",
      " |          Only consider the ``ensemble_nbest`` models when building an\n",
      " |          ensemble. This is inspired by a concept called library pruning\n",
      " |          introduced in `Getting Most out of Ensemble Selection`. This\n",
      " |          is independent of the ``ensemble_class`` argument and this\n",
      " |          pruning step is done prior to constructing an ensemble.\n",
      " |      \n",
      " |      ensemble_class : Type[AbstractEnsemble] | \"default\", optional (default=\"default\")\n",
      " |          Class implementing the post-hoc ensemble algorithm. Set to\n",
      " |          ``None`` to disable ensemble building or use class:`SingleBest`\n",
      " |          to obtain only use the single best model instead of an\n",
      " |          ensemble.\n",
      " |      \n",
      " |          If set to \"default\" it will use :class:`EnsembleSelection` for\n",
      " |          single-objective problems and :class:`MultiObjectiveDummyEnsemble`\n",
      " |          for multi-objective problems.\n",
      " |      \n",
      " |      metric: Scorer | Sequence[Scorer] | None = None\n",
      " |          A metric or list of metrics to score the ensemble with\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_pipeline(self, X: 'SUPPORTED_FEAT_TYPES', y: 'Union[SUPPORTED_TARGET_TYPES, spmatrix]', config: 'Union[Configuration, Dict[str, Union[str, float, int]]]', dataset_name: 'Optional[str]' = None, X_test: 'Optional[SUPPORTED_FEAT_TYPES]' = None, y_test: 'Optional[Union[SUPPORTED_TARGET_TYPES, spmatrix]]' = None, feat_type: 'Optional[List[str]]' = None, *args, **kwargs: 'Dict') -> 'Tuple[Optional[BasePipeline], RunInfo, RunValue]'\n",
      " |      Fits and individual pipeline configuration and returns\n",
      " |      the result to the user.\n",
      " |      \n",
      " |      The Estimator constraints are honored, for example the resampling\n",
      " |      strategy, or memory constraints, unless directly provided to the method.\n",
      " |      By default, this method supports the same signature as fit(), and any extra\n",
      " |      arguments are redirected to the TAE evaluation function, which allows for\n",
      " |      further customization while building a pipeline.\n",
      " |      \n",
      " |      Any additional argument provided is directly passed to the\n",
      " |      worker exercising the run.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: array-like, shape = (n_samples, n_features)\n",
      " |          The features used for training\n",
      " |      y: array-like\n",
      " |          The labels used for training\n",
      " |      X_test: Optionalarray-like, shape = (n_samples, n_features)\n",
      " |          If provided, the testing performance will be tracked on this features.\n",
      " |      y_test: array-like\n",
      " |          If provided, the testing performance will be tracked on this labels\n",
      " |      config: Union[Configuration,  Dict[str, Union[str, float, int]]]\n",
      " |          A configuration object used to define the pipeline steps.\n",
      " |          If a dict is passed, a configuration is created based on this dict.\n",
      " |      dataset_name: Optional[str]\n",
      " |          Name that will be used to tag the Auto-Sklearn run and identify the\n",
      " |          Auto-Sklearn run\n",
      " |      feat_type : list, optional (default=None)\n",
      " |          List of str of `len(X.shape[1])` describing the attribute type.\n",
      " |          Possible types are `Categorical` and `Numerical`. `Categorical`\n",
      " |          attributes will be automatically One-Hot encoded. The values\n",
      " |          used for a categorical attribute must be integers, obtained for\n",
      " |          example by `sklearn.preprocessing.LabelEncoder\n",
      " |          <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>`_.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pipeline: Optional[BasePipeline]\n",
      " |          The fitted pipeline. In case of failure while fitting the pipeline,\n",
      " |          a None is returned.\n",
      " |      run_info: RunInFo\n",
      " |          A named tuple that contains the configuration launched\n",
      " |      run_value: RunValue\n",
      " |          A named tuple that contains the result of the run\n",
      " |  \n",
      " |  get_configuration_space(self, X: 'SUPPORTED_FEAT_TYPES', y: 'Union[SUPPORTED_TARGET_TYPES, spmatrix]', X_test: 'Optional[SUPPORTED_FEAT_TYPES]' = None, y_test: 'Optional[Union[SUPPORTED_TARGET_TYPES, spmatrix]]' = None, dataset_name: 'Optional[str]' = None, feat_type: 'Optional[List[str]]' = None) -> 'ConfigurationSpace'\n",
      " |      Returns the Configuration Space object, from which Auto-Sklearn\n",
      " |      will sample configurations and build pipelines.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Array with the training features, used to get characteristics like\n",
      " |          data sparsity\n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          Array with the problem labels\n",
      " |      X_test : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Array with features used for performance estimation\n",
      " |      y_test : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          Array with the problem labels for the testing split\n",
      " |      dataset_name: Optional[str]\n",
      " |          A string to tag the Auto-Sklearn run\n",
      " |  \n",
      " |  get_models_with_weights(self)\n",
      " |      Return a list of the final ensemble found by auto-sklearn.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      [(weight_1, model_1), ..., (weight_n, model_n)]\n",
      " |  \n",
      " |  get_pareto_set(self) -> 'Sequence[VotingClassifier | VotingRegressor]'\n",
      " |  \n",
      " |  leaderboard(self, detailed: 'bool' = False, ensemble_only: 'bool' = True, top_k: \"Union[int, Literal['all']]\" = 'all', sort_by: 'str' = 'cost', sort_order: \"Literal['auto', 'ascending', 'descending']\" = 'auto', include: 'Optional[Union[str, Iterable[str]]]' = None) -> 'pd.DataFrame'\n",
      " |      Returns a pandas table of results for all evaluated models.\n",
      " |      \n",
      " |      Gives an overview of all models trained during the search process along\n",
      " |      with various statistics about their training.\n",
      " |      \n",
      " |      The available statistics are:\n",
      " |      \n",
      " |      **Simple**:\n",
      " |      \n",
      " |      * ``\"model_id\"`` - The id given to a model by ``autosklearn``.\n",
      " |      * ``\"rank\"`` - The rank of the model based on it's ``\"cost\"``.\n",
      " |      * ``\"ensemble_weight\"`` - The weight given to the model in the ensemble.\n",
      " |      * ``\"type\"`` - The type of classifier/regressor used.\n",
      " |      * ``\"cost\"`` - The loss of the model on the validation set.\n",
      " |      * ``\"duration\"`` - Length of time the model was optimized for.\n",
      " |      \n",
      " |      **Detailed**:\n",
      " |      The detailed view includes all of the simple statistics along with the\n",
      " |      following.\n",
      " |      \n",
      " |      * ``\"config_id\"`` - The id used by SMAC for optimization.\n",
      " |      * ``\"budget\"`` - How much budget was allocated to this model.\n",
      " |      * ``\"status\"`` - The return status of training the model with SMAC.\n",
      " |      * ``\"train_loss\"`` - The loss of the model on the training set.\n",
      " |      * ``\"balancing_strategy\"`` - The balancing strategy used for data preprocessing.\n",
      " |      * ``\"start_time\"`` - Time the model began being optimized\n",
      " |      * ``\"end_time\"`` - Time the model ended being optimized\n",
      " |      * ``\"data_preprocessors\"`` - The preprocessors used on the data\n",
      " |      * ``\"feature_preprocessors\"`` - The preprocessors for features types\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      detailed: bool = False\n",
      " |          Whether to give detailed information or just a simple overview.\n",
      " |      \n",
      " |      ensemble_only: bool = True\n",
      " |          Whether to view only models included in the ensemble or all models\n",
      " |          trained.\n",
      " |      \n",
      " |      top_k: int or \"all\" = \"all\"\n",
      " |          How many models to display.\n",
      " |      \n",
      " |      sort_by: str = 'cost'\n",
      " |          What column to sort by. If that column is not present, the\n",
      " |          sorting defaults to the ``\"model_id\"`` index column.\n",
      " |      \n",
      " |          Defaults to the metric optimized. Sort by the first objective\n",
      " |          in case of a multi-objective optimization problem\n",
      " |      \n",
      " |      sort_order: \"auto\" or \"ascending\" or \"descending\" = \"auto\"\n",
      " |          Which sort order to apply to the ``sort_by`` column. If left\n",
      " |          as ``\"auto\"``, it will sort by a sensible default where \"better\" is\n",
      " |          on top, otherwise defaulting to the pandas default for\n",
      " |          `DataFrame.sort_values`_ if there is no obvious \"better\".\n",
      " |      \n",
      " |          .. _DataFrame.sort_values: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
      " |      \n",
      " |      include: Optional[str or Iterable[str]]\n",
      " |          Items to include, other items not specified will be excluded.\n",
      " |          The exception is the ``\"model_id\"`` index column which is always included.\n",
      " |      \n",
      " |          If left as ``None``, it will resort back to using the ``detailed``\n",
      " |          param to decide the columns to include.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pd.DataFrame\n",
      " |          A dataframe of statistics for the models, ordered by ``sort_by``.\n",
      " |  \n",
      " |  refit(self, X, y)\n",
      " |      Refit all models found with fit to new data.\n",
      " |      \n",
      " |      Necessary when using cross-validation. During training, auto-sklearn\n",
      " |      fits each model k times on the dataset, but does not keep any trained\n",
      " |      model and can therefore not be used to predict for new data points.\n",
      " |      This methods fits all models found during a call to fit on the data\n",
      " |      given. This method may also be used together with holdout to avoid\n",
      " |      only using 66% of the training data to fit the final model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The targets.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      self\n",
      " |  \n",
      " |  score(self, X, y)\n",
      " |  \n",
      " |  show_models(self)\n",
      " |      Returns a dictionary containing dictionaries of ensemble models.\n",
      " |      \n",
      " |      Each model in the ensemble can be accessed by giving its ``model_id`` as key.\n",
      " |      \n",
      " |      A model dictionary contains the following:\n",
      " |      \n",
      " |      * ``\"model_id\"`` - The id given to a model by ``autosklearn``.\n",
      " |      \n",
      " |      * ``\"rank\"`` - The rank of the model based on it's ``\"cost\"``.\n",
      " |      \n",
      " |      * ``\"cost\"`` - The loss of the model on the validation set.\n",
      " |      \n",
      " |      * ``\"ensemble_weight\"`` - The weight given to the model in the ensemble.\n",
      " |      \n",
      " |      * ``\"voting_model\"`` - The ``cv_voting_ensemble`` model (for 'cv' resampling).\n",
      " |      \n",
      " |      * ``\"estimators\"`` - List of models (dicts) in ``cv_voting_ensemble``\n",
      " |          ('cv' resampling).\n",
      " |      \n",
      " |      * ``\"data_preprocessor\"`` - The preprocessor used on the data.\n",
      " |      \n",
      " |      * ``\"balancing\"`` - The balancing used on the data (for classification).\n",
      " |      \n",
      " |      * ``\"feature_preprocessor\"`` - The preprocessor for features types.\n",
      " |      \n",
      " |      * ``\"classifier\"`` / ``\"regressor\"``\n",
      " |        - The autosklearn wrapped classifier or regressor.\n",
      " |      \n",
      " |      * ``\"sklearn_classifier\"`` or ``\"sklearn_regressor\"``\n",
      " |        - The sklearn classifier or regressor.\n",
      " |      \n",
      " |      **Example**\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          import sklearn.datasets\n",
      " |          import sklearn.metrics\n",
      " |          import autosklearn.regression\n",
      " |      \n",
      " |          X, y = sklearn.datasets.load_diabetes(return_X_y=True)\n",
      " |      \n",
      " |          automl = autosklearn.regression.AutoSklearnRegressor(\n",
      " |              time_left_for_this_task=120\n",
      " |              )\n",
      " |          automl.fit(X_train, y_train, dataset_name='diabetes')\n",
      " |      \n",
      " |          ensemble_dict = automl.show_models()\n",
      " |          print(ensemble_dict)\n",
      " |      \n",
      " |      Output:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          {\n",
      " |              25: {'model_id': 25.0,\n",
      " |                   'rank': 1,\n",
      " |                   'cost': 0.43667876507897496,\n",
      " |                   'ensemble_weight': 0.38,\n",
      " |                   'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing....>,\n",
      " |                   'feature_preprocessor': <autosklearn.pipeline.components....>,\n",
      " |                   'regressor': <autosklearn.pipeline.components.regression....>,\n",
      " |                   'sklearn_regressor': SGDRegressor(alpha=0.0006517033225329654,...)\n",
      " |                  },\n",
      " |              6: {'model_id': 6.0,\n",
      " |                  'rank': 2,\n",
      " |                  'cost': 0.4550418898836528,\n",
      " |                  'ensemble_weight': 0.3,\n",
      " |                  'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing....>,\n",
      " |                  'feature_preprocessor': <autosklearn.pipeline.components....>,\n",
      " |                  'regressor': <autosklearn.pipeline.components.regression....>,\n",
      " |                  'sklearn_regressor': ARDRegression(alpha_1=0.0003701926442639788,...)\n",
      " |                  }...\n",
      " |          }\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dict(int, Any) : dictionary of length = number of models in the ensemble\n",
      " |          A dictionary of models in the ensemble, where ``model_id`` is the key.\n",
      " |  \n",
      " |  sprint_statistics(self)\n",
      " |      Return the following statistics of the training result:\n",
      " |      \n",
      " |      - dataset name\n",
      " |      - metric used\n",
      " |      - best validation score\n",
      " |      - number of target algorithm runs\n",
      " |      - number of successful target algorithm runs\n",
      " |      - number of crashed target algorithm runs\n",
      " |      - number of target algorithm runs that exceeded the memory limit\n",
      " |      - number of target algorithm runs that exceeded the time limit\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from AutoSklearnEstimator:\n",
      " |  \n",
      " |  cv_results_\n",
      " |  \n",
      " |  fANOVA_input_\n",
      " |  \n",
      " |  performance_over_time_\n",
      " |  \n",
      " |  trajectory_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(autosklearn.classification.AutoSklearnClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59678,)\n",
      "                           VPREB3     RGS13    S100A6      EMP3  TNFRSF13B  \\\n",
      "Unnamed: 0                                                                   \n",
      "HD1_TN_AAACCCAAGAGTTGCG  2.193819  1.754687  0.000000  0.953155   0.000000   \n",
      "HD1_TN_AAACCCAAGCCTGCCA  1.366577  0.606951  0.000000  0.000000   0.348834   \n",
      "HD1_TN_AAACCCAAGGAGCTGT  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "HD1_TN_AAACCCAAGTATTCCG  0.000000  0.773783  0.773783  0.000000   0.000000   \n",
      "HD1_TN_AAACCCAAGTGGATAT  1.600064  1.935864  0.000000  0.000000   0.000000   \n",
      "...                           ...       ...       ...       ...        ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG  0.000000  0.000000  1.266380  0.614823   1.657589   \n",
      "HD9_BM_TTTGTTGTCCTTATGT  0.000000  0.000000  2.382285  1.453144   0.000000   \n",
      "HD9_BM_TTTGTTGTCGAGAACG  0.000000  0.000000  1.048450  1.048450   0.000000   \n",
      "HD9_BM_TTTGTTGTCTCACTCG  0.000000  0.000000  0.957807  0.000000   0.790492   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC  0.000000  0.000000  3.593586  1.105526   0.698337   \n",
      "\n",
      "                             CD48    RASSF6   HLA-DRA  HLA-DRB1    LGALS1  \\\n",
      "Unnamed: 0                                                                  \n",
      "HD1_TN_AAACCCAAGAGTTGCG  0.000000  0.000000  2.919535  3.335625  1.432166   \n",
      "HD1_TN_AAACCCAAGCCTGCCA  0.348834  1.254044  2.189515  2.634752  0.606951   \n",
      "HD1_TN_AAACCCAAGGAGCTGT  0.000000  0.949060  0.000000  2.351345  0.000000   \n",
      "HD1_TN_AAACCCAAGTATTCCG  0.773783  0.000000  0.000000  1.504934  1.204743   \n",
      "HD1_TN_AAACCCAAGTGGATAT  0.000000  0.000000  2.387237  2.822176  0.000000   \n",
      "...                           ...       ...       ...       ...       ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG  1.938066  0.000000  0.000000  0.614823  0.992755   \n",
      "HD9_BM_TTTGTTGTCCTTATGT  0.000000  0.000000  0.000000  0.000000  2.382285   \n",
      "HD9_BM_TTTGTTGTCGAGAACG  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "HD9_BM_TTTGTTGTCTCACTCG  1.101098  0.000000  1.101098  1.101098  0.589445   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC  1.394069  0.000000  0.698337  0.698337  1.394069   \n",
      "\n",
      "                         ...  NCMAP-DT    ZNF709  AKAP6  KCNC4  MAFA  ZNF540  \\\n",
      "Unnamed: 0               ...                                                   \n",
      "HD1_TN_AAACCCAAGAGTTGCG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGCCTGCCA  ...       0.0  0.348834    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGGAGCTGT  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGTATTCCG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGTGGATAT  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "...                      ...       ...       ...    ...    ...   ...     ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCCTTATGT  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCGAGAACG  ...       0.0  0.655763    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCTCACTCG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "\n",
      "                         DEGS2  HSPA12A  ADCY9                    celltype  \n",
      "Unnamed: 0                                                                  \n",
      "HD1_TN_AAACCCAAGAGTTGCG    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGCCTGCCA    0.0      0.0    0.0   tonsil_cycling_plasmacell  \n",
      "HD1_TN_AAACCCAAGGAGCTGT    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGTATTCCG    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGTGGATAT    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "...                        ...      ...    ...                         ...  \n",
      "HD9_BM_TTTGTTGTCCTATTTG    0.0      0.0    0.0  blood_highCXCR4_plasmacell  \n",
      "HD9_BM_TTTGTTGTCCTTATGT    0.0      0.0    0.0        blood_end_plasmacell  \n",
      "HD9_BM_TTTGTTGTCGAGAACG    0.0      0.0    0.0   blood_lowCXCR4_plasmacell  \n",
      "HD9_BM_TTTGTTGTCTCACTCG    0.0      0.0    0.0    blood_cycling_plasmacell  \n",
      "HD9_BM_TTTGTTGTCTTGTGCC    0.0      0.0    0.0   blood_lowCXCR4_plasmacell  \n",
      "\n",
      "[59678 rows x 12893 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['tonsil_highCD74_plasmacell', 'tonsil_cycling_plasmacell',\n",
       "       'tonsil_lowCD74_plasmacell', 'blood_lowCXCR4_plasmacell',\n",
       "       'blood_highCXCR4_plasmacell', 'blood_end_plasmacell',\n",
       "       'blood_cycling_plasmacell', 'blood_plasmablast'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(19980501)\n",
    "plasma=pd.read_csv('plasmacell1.csv')\n",
    "print(plasma['celltype'].shape)\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "print(plasma)\n",
    "plasma['celltype'].unique()\n",
    "\n",
    "testset=random.sample(range(1,59678),12000,)\n",
    "testset= plasma.iloc[testset,]\n",
    "test_celltype=testset.celltype\n",
    "testset = testset.drop('celltype',axis=1)\n",
    "\n",
    "trainset = plasma.drop(testset.index,axis=0)\n",
    "train_celltype=trainset.celltype\n",
    "trainset = trainset.drop('celltype',axis=1)\n",
    "print(test_celltype.value_counts())\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=28800,memory_limit=128000,n_jobs=4,per_run_time_limit=7200)\n",
    "\n",
    "train_celltype.values\n",
    "\n",
    "automl.fit(trainset,train_celltype.values)\n",
    "\n",
    "print(automl.leaderboard())\n",
    "\n",
    "predictions = automl.predict(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['automl_Normalized.jl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib as jl\n",
    "jl.dump(automl, 'automl_Normalized.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9451666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score:\", sklearn.metrics.accuracy_score(test_celltype.values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl= jl.load('automl_Normalized.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59678,)\n",
      "                           VPREB3     RGS13    S100A6      EMP3  TNFRSF13B  \\\n",
      "Unnamed: 0                                                                   \n",
      "HD1_TN_AAACCCAAGAGTTGCG  2.193819  1.754687  0.000000  0.953155   0.000000   \n",
      "HD1_TN_AAACCCAAGCCTGCCA  1.366577  0.606951  0.000000  0.000000   0.348834   \n",
      "HD1_TN_AAACCCAAGGAGCTGT  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "HD1_TN_AAACCCAAGTATTCCG  0.000000  0.773783  0.773783  0.000000   0.000000   \n",
      "HD1_TN_AAACCCAAGTGGATAT  1.600064  1.935864  0.000000  0.000000   0.000000   \n",
      "...                           ...       ...       ...       ...        ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG  0.000000  0.000000  1.266380  0.614823   1.657589   \n",
      "HD9_BM_TTTGTTGTCCTTATGT  0.000000  0.000000  2.382285  1.453144   0.000000   \n",
      "HD9_BM_TTTGTTGTCGAGAACG  0.000000  0.000000  1.048450  1.048450   0.000000   \n",
      "HD9_BM_TTTGTTGTCTCACTCG  0.000000  0.000000  0.957807  0.000000   0.790492   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC  0.000000  0.000000  3.593586  1.105526   0.698337   \n",
      "\n",
      "                             CD48    RASSF6   HLA-DRA  HLA-DRB1    LGALS1  \\\n",
      "Unnamed: 0                                                                  \n",
      "HD1_TN_AAACCCAAGAGTTGCG  0.000000  0.000000  2.919535  3.335625  1.432166   \n",
      "HD1_TN_AAACCCAAGCCTGCCA  0.348834  1.254044  2.189515  2.634752  0.606951   \n",
      "HD1_TN_AAACCCAAGGAGCTGT  0.000000  0.949060  0.000000  2.351345  0.000000   \n",
      "HD1_TN_AAACCCAAGTATTCCG  0.773783  0.000000  0.000000  1.504934  1.204743   \n",
      "HD1_TN_AAACCCAAGTGGATAT  0.000000  0.000000  2.387237  2.822176  0.000000   \n",
      "...                           ...       ...       ...       ...       ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG  1.938066  0.000000  0.000000  0.614823  0.992755   \n",
      "HD9_BM_TTTGTTGTCCTTATGT  0.000000  0.000000  0.000000  0.000000  2.382285   \n",
      "HD9_BM_TTTGTTGTCGAGAACG  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "HD9_BM_TTTGTTGTCTCACTCG  1.101098  0.000000  1.101098  1.101098  0.589445   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC  1.394069  0.000000  0.698337  0.698337  1.394069   \n",
      "\n",
      "                         ...  NCMAP-DT    ZNF709  AKAP6  KCNC4  MAFA  ZNF540  \\\n",
      "Unnamed: 0               ...                                                   \n",
      "HD1_TN_AAACCCAAGAGTTGCG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGCCTGCCA  ...       0.0  0.348834    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGGAGCTGT  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGTATTCCG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD1_TN_AAACCCAAGTGGATAT  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "...                      ...       ...       ...    ...    ...   ...     ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCCTTATGT  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCGAGAACG  ...       0.0  0.655763    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCTCACTCG  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC  ...       0.0  0.000000    0.0    0.0   0.0     0.0   \n",
      "\n",
      "                         DEGS2  HSPA12A  ADCY9                    celltype  \n",
      "Unnamed: 0                                                                  \n",
      "HD1_TN_AAACCCAAGAGTTGCG    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGCCTGCCA    0.0      0.0    0.0   tonsil_cycling_plasmacell  \n",
      "HD1_TN_AAACCCAAGGAGCTGT    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGTATTCCG    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGTGGATAT    0.0      0.0    0.0  tonsil_highCD74_plasmacell  \n",
      "...                        ...      ...    ...                         ...  \n",
      "HD9_BM_TTTGTTGTCCTATTTG    0.0      0.0    0.0  blood_highCXCR4_plasmacell  \n",
      "HD9_BM_TTTGTTGTCCTTATGT    0.0      0.0    0.0        blood_end_plasmacell  \n",
      "HD9_BM_TTTGTTGTCGAGAACG    0.0      0.0    0.0   blood_lowCXCR4_plasmacell  \n",
      "HD9_BM_TTTGTTGTCTCACTCG    0.0      0.0    0.0    blood_cycling_plasmacell  \n",
      "HD9_BM_TTTGTTGTCTTGTGCC    0.0      0.0    0.0   blood_lowCXCR4_plasmacell  \n",
      "\n",
      "[59678 rows x 12893 columns]\n"
     ]
    }
   ],
   "source": [
    "random.seed(19980501)\n",
    "plasma=pd.read_csv('plasmacell1.csv')\n",
    "print(plasma['celltype'].shape)\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "print(plasma)\n",
    "plasma['celltype'].unique()\n",
    "\n",
    "testset=random.sample(range(1,59678),12000,)\n",
    "testset= plasma.iloc[testset,]\n",
    "test_celltype=testset.celltype\n",
    "testset = testset.drop('celltype',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = automl.predict_proba(testset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03418633, 0.02105156, 0.49208104, ..., 0.01909713, 0.04084255,\n",
       "        0.04583429],\n",
       "       [0.01701122, 0.01596689, 0.02843505, ..., 0.03168928, 0.72878177,\n",
       "        0.03829239],\n",
       "       [0.0313759 , 0.02020046, 0.09715606, ..., 0.01579634, 0.02412456,\n",
       "        0.02019644],\n",
       "       ...,\n",
       "       [0.61829998, 0.01826646, 0.02176424, ..., 0.04242047, 0.01844022,\n",
       "        0.01730832],\n",
       "       [0.01499896, 0.01615131, 0.03533476, ..., 0.04513889, 0.77364346,\n",
       "        0.07627904],\n",
       "       [0.01571973, 0.02453046, 0.08642053, ..., 0.01942939, 0.08107038,\n",
       "        0.73206059]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034186</td>\n",
       "      <td>0.021052</td>\n",
       "      <td>0.492081</td>\n",
       "      <td>0.310012</td>\n",
       "      <td>0.036895</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>0.040843</td>\n",
       "      <td>0.045834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017011</td>\n",
       "      <td>0.015967</td>\n",
       "      <td>0.028435</td>\n",
       "      <td>0.106348</td>\n",
       "      <td>0.033476</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>0.728782</td>\n",
       "      <td>0.038292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031376</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.097156</td>\n",
       "      <td>0.753259</td>\n",
       "      <td>0.037891</td>\n",
       "      <td>0.015796</td>\n",
       "      <td>0.024125</td>\n",
       "      <td>0.020196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.161860</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>0.040881</td>\n",
       "      <td>0.600472</td>\n",
       "      <td>0.063040</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.054237</td>\n",
       "      <td>0.032614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.040897</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.772880</td>\n",
       "      <td>0.035272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>0.020707</td>\n",
       "      <td>0.021652</td>\n",
       "      <td>0.041384</td>\n",
       "      <td>0.028295</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.041363</td>\n",
       "      <td>0.610311</td>\n",
       "      <td>0.208533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>0.015238</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.038792</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.017369</td>\n",
       "      <td>0.043271</td>\n",
       "      <td>0.794633</td>\n",
       "      <td>0.054172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>0.618300</td>\n",
       "      <td>0.018266</td>\n",
       "      <td>0.021764</td>\n",
       "      <td>0.217936</td>\n",
       "      <td>0.045564</td>\n",
       "      <td>0.042420</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.017308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.016151</td>\n",
       "      <td>0.035335</td>\n",
       "      <td>0.019236</td>\n",
       "      <td>0.019218</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>0.773643</td>\n",
       "      <td>0.076279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>0.015720</td>\n",
       "      <td>0.024530</td>\n",
       "      <td>0.086421</td>\n",
       "      <td>0.024278</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>0.019429</td>\n",
       "      <td>0.081070</td>\n",
       "      <td>0.732061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.034186  0.021052  0.492081  0.310012  0.036895  0.019097  0.040843   \n",
       "1      0.017011  0.015967  0.028435  0.106348  0.033476  0.031689  0.728782   \n",
       "2      0.031376  0.020200  0.097156  0.753259  0.037891  0.015796  0.024125   \n",
       "3      0.161860  0.021143  0.040881  0.600472  0.063040  0.025754  0.054237   \n",
       "4      0.017900  0.018541  0.040819  0.040897  0.026614  0.047078  0.772880   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11995  0.020707  0.021652  0.041384  0.028295  0.027755  0.041363  0.610311   \n",
       "11996  0.015238  0.016082  0.038792  0.020443  0.017369  0.043271  0.794633   \n",
       "11997  0.618300  0.018266  0.021764  0.217936  0.045564  0.042420  0.018440   \n",
       "11998  0.014999  0.016151  0.035335  0.019236  0.019218  0.045139  0.773643   \n",
       "11999  0.015720  0.024530  0.086421  0.024278  0.016491  0.019429  0.081070   \n",
       "\n",
       "              7  \n",
       "0      0.045834  \n",
       "1      0.038292  \n",
       "2      0.020196  \n",
       "3      0.032614  \n",
       "4      0.035272  \n",
       "...         ...  \n",
       "11995  0.208533  \n",
       "11996  0.054172  \n",
       "11997  0.017308  \n",
       "11998  0.076279  \n",
       "11999  0.732061  \n",
       "\n",
       "[12000 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = pd.DataFrame(probabilities)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.index=testset.index\n",
    "probabilities.to_csv('mmself1_proba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma=pd.read_csv('mmself1.csv')\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "predictions = automl.predict(plasma)\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "predictions.value_counts()\n",
    "\n",
    "predictions.index=plasma.index\n",
    "predictions.value_counts()\n",
    "predictions.to_csv('mmself1_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59678,)\n",
      "                         AL627309.1  AL627309.5  AP006222.2  LINC01409  \\\n",
      "Unnamed: 0                                                               \n",
      "HD1_TN_AAACCCAAGAGTTGCG           0           0           0          0   \n",
      "HD1_TN_AAACCCAAGCCTGCCA           0           0           0          0   \n",
      "HD1_TN_AAACCCAAGGAGCTGT           0           0           0          0   \n",
      "HD1_TN_AAACCCAAGTATTCCG           0           0           0          0   \n",
      "HD1_TN_AAACCCAAGTGGATAT           0           0           0          0   \n",
      "...                             ...         ...         ...        ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG           0           0           0          0   \n",
      "HD9_BM_TTTGTTGTCCTTATGT           0           0           0          0   \n",
      "HD9_BM_TTTGTTGTCGAGAACG           0           0           0          0   \n",
      "HD9_BM_TTTGTTGTCTCACTCG           0           1           0          1   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC           0           0           0          0   \n",
      "\n",
      "                         LINC01128  LINC00115  FAM41C  AL645608.2  SAMD11  \\\n",
      "Unnamed: 0                                                                  \n",
      "HD1_TN_AAACCCAAGAGTTGCG          0          0       0           0       0   \n",
      "HD1_TN_AAACCCAAGCCTGCCA          0          0       0           0       0   \n",
      "HD1_TN_AAACCCAAGGAGCTGT          0          0       0           0       0   \n",
      "HD1_TN_AAACCCAAGTATTCCG          0          0       0           0       0   \n",
      "HD1_TN_AAACCCAAGTGGATAT          0          0       0           0       0   \n",
      "...                            ...        ...     ...         ...     ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG          0          0       0           0       0   \n",
      "HD9_BM_TTTGTTGTCCTTATGT          0          0       0           0       0   \n",
      "HD9_BM_TTTGTTGTCGAGAACG          0          0       0           0       0   \n",
      "HD9_BM_TTTGTTGTCTCACTCG          1          0       0           0       0   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC          0          0       0           0       0   \n",
      "\n",
      "                         NOC2L  ...  AL592183.1  AC240274.1  AC004556.3  \\\n",
      "Unnamed: 0                      ...                                       \n",
      "HD1_TN_AAACCCAAGAGTTGCG      1  ...           0           0           0   \n",
      "HD1_TN_AAACCCAAGCCTGCCA      0  ...           0           0           0   \n",
      "HD1_TN_AAACCCAAGGAGCTGT      0  ...           0           0           0   \n",
      "HD1_TN_AAACCCAAGTATTCCG      1  ...           0           0           0   \n",
      "HD1_TN_AAACCCAAGTGGATAT      0  ...           0           0           0   \n",
      "...                        ...  ...         ...         ...         ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG      0  ...           0           0           0   \n",
      "HD9_BM_TTTGTTGTCCTTATGT      0  ...           0           0           0   \n",
      "HD9_BM_TTTGTTGTCGAGAACG      0  ...           0           0           0   \n",
      "HD9_BM_TTTGTTGTCTCACTCG      1  ...           0           0           0   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC      0  ...           0           0           1   \n",
      "\n",
      "                         AC233755.2  AC233755.1  AC136616.3  AC136616.2  \\\n",
      "Unnamed: 0                                                                \n",
      "HD1_TN_AAACCCAAGAGTTGCG           0           0           0           0   \n",
      "HD1_TN_AAACCCAAGCCTGCCA           0           0           0           0   \n",
      "HD1_TN_AAACCCAAGGAGCTGT           0           0           0           0   \n",
      "HD1_TN_AAACCCAAGTATTCCG           0           0           0           0   \n",
      "HD1_TN_AAACCCAAGTGGATAT           0           0           0           0   \n",
      "...                             ...         ...         ...         ...   \n",
      "HD9_BM_TTTGTTGTCCTATTTG           0           0           0           0   \n",
      "HD9_BM_TTTGTTGTCCTTATGT           0           0           0           0   \n",
      "HD9_BM_TTTGTTGTCGAGAACG           1           0           0           0   \n",
      "HD9_BM_TTTGTTGTCTCACTCG           0           0           0           1   \n",
      "HD9_BM_TTTGTTGTCTTGTGCC           0           0           0           0   \n",
      "\n",
      "                         AC141272.1  AC007325.4                    celltype  \n",
      "Unnamed: 0                                                                   \n",
      "HD1_TN_AAACCCAAGAGTTGCG           0           0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGCCTGCCA           1           0   tonsil_cycling_plasmacell  \n",
      "HD1_TN_AAACCCAAGGAGCTGT           0           0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGTATTCCG           0           0  tonsil_highCD74_plasmacell  \n",
      "HD1_TN_AAACCCAAGTGGATAT           0           0  tonsil_highCD74_plasmacell  \n",
      "...                             ...         ...                         ...  \n",
      "HD9_BM_TTTGTTGTCCTATTTG           0           0  blood_highCXCR4_plasmacell  \n",
      "HD9_BM_TTTGTTGTCCTTATGT           0           0        blood_end_plasmacell  \n",
      "HD9_BM_TTTGTTGTCGAGAACG           0           0   blood_lowCXCR4_plasmacell  \n",
      "HD9_BM_TTTGTTGTCTCACTCG           0           0    blood_cycling_plasmacell  \n",
      "HD9_BM_TTTGTTGTCTTGTGCC           0           0   blood_lowCXCR4_plasmacell  \n",
      "\n",
      "[59678 rows x 18525 columns]\n",
      "tonsil_highCD74_plasmacell    5480\n",
      "blood_highCXCR4_plasmacell    2479\n",
      "tonsil_lowCD74_plasmacell     1645\n",
      "blood_lowCXCR4_plasmacell     1368\n",
      "blood_end_plasmacell           420\n",
      "blood_cycling_plasmacell       346\n",
      "blood_plasmablast              212\n",
      "tonsil_cycling_plasmacell       50\n",
      "Name: celltype, dtype: int64\n",
      "[WARNING] [2024-06-05 17:34:39,432:Client-EnsembleBuilder] No runs were available to build an ensemble from\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoSklearnClassifier(ensemble_class=<class 'autosklearn.ensembles.ensemble_selection.EnsembleSelection'>,\n",
       "                      memory_limit=128000, n_jobs=4, per_run_time_limit=7200,\n",
       "                      time_left_for_this_task=28800)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(19980501)\n",
    "plasma=pd.read_csv('plasmacell2.csv')\n",
    "print(plasma['celltype'].shape)\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "print(plasma)\n",
    "plasma['celltype'].unique()\n",
    "\n",
    "testset=random.sample(range(1,59678),12000,)\n",
    "testset= plasma.iloc[testset,]\n",
    "test_celltype=testset.celltype\n",
    "testset = testset.drop('celltype',axis=1)\n",
    "\n",
    "trainset = plasma.drop(testset.index,axis=0)\n",
    "train_celltype=trainset.celltype\n",
    "trainset = trainset.drop('celltype',axis=1)\n",
    "print(test_celltype.value_counts())\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=28800,memory_limit=128000,n_jobs=4,per_run_time_limit=7200)\n",
    "\n",
    "train_celltype.values\n",
    "\n",
    "automl.fit(trainset,train_celltype.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank  ensemble_weight                type      cost     duration\n",
      "model_id                                                                  \n",
      "4            1             0.22  passive_aggressive  0.064192  3662.505740\n",
      "7            2             0.18                 lda  0.068705  3116.798043\n",
      "24           3             0.22       liblinear_svc  0.072709  1921.763673\n",
      "15           4             0.06       random_forest  0.080081  4937.300807\n",
      "6            5             0.06       random_forest  0.081416  5354.270399\n",
      "18           6             0.12   gradient_boosting  0.083069  4075.035921\n",
      "14           7             0.02       random_forest  0.101500  6303.438540\n",
      "2            8             0.02       random_forest  0.121584  3686.203751\n",
      "17           9             0.10                 lda  0.145418  5364.844817\n",
      "Accuracy score: 0.9398333333333333\n"
     ]
    }
   ],
   "source": [
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9398333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions = automl.predict(testset)\n",
    "print(\"Accuracy score:\", \n",
    "      sklearn.metrics.accuracy_score(test_celltype.values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['automl_count.jl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib as jl\n",
    "jl.dump(automl, 'automl_count.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma=pd.read_csv('mmself2.csv')\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "predictions = automl.predict(plasma)\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "predictions.value_counts()\n",
    "\n",
    "predictions.index=plasma.index\n",
    "predictions.value_counts()\n",
    "predictions.to_csv('mmself2_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tonsil_lowCD74_plasmacell     3052\n",
       "blood_highCXCR4_plasmacell    1224\n",
       "blood_end_plasmacell           957\n",
       "blood_cycling_plasmacell       351\n",
       "blood_lowCXCR4_plasmacell      314\n",
       "blood_plasmablast              213\n",
       "tonsil_highCD74_plasmacell     102\n",
       "tonsil_cycling_plasmacell       61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jl\n",
    "automl= jl.load('automl_Normalized.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma=pd.read_csv('verify.csv')\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "predictions = automl.predict(plasma)\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "predictions.value_counts()\n",
    "\n",
    "predictions.index=plasma.index\n",
    "predictions.value_counts()\n",
    "predictions.to_csv('verify_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jl\n",
    "automl= jl.load('automl_Normalized.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma=pd.read_csv('PRJ.csv')\n",
    "plasma.index=[plasma['Unnamed: 0']]\n",
    "plasma=plasma.drop('Unnamed: 0',axis=1)\n",
    "predictions = automl.predict(plasma)\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "predictions.value_counts()\n",
    "\n",
    "predictions.index=plasma.index\n",
    "predictions.value_counts()\n",
    "predictions.to_csv('PRJ_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
